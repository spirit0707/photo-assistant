import streamlit as st
import cv2
import numpy as np
from PIL import Image
from core.analysis.brightness import analyze_brightness
from core.analysis.sharpness import analyze_sharpness
from core.analysis.contrast import analyze_contrast
from core.analysis.saturation import analyze_saturation
from core.analysis.overlay import draw_grid, apply_genre_filter, apply_scene_filter
from core.analysis.composition import analyze_composition
from core.detection.yolo_detector import detect_objects, detect_categories
from core.analysis.pose import analyze_pose, draw_pose
from core.analysis.blur_detection import detect_blur
from core.ml.genre.inference import predict_genre_ml
from core.features.extractors import extract_features, GENRE_LABELS, GENRE_TO_IDX, GENRE_MODEL_TO_RU
from core.ml.quality.inference import predict_quality_ml, explain_quality, predict_quality_cnn
from core.visualization.cluster_viz import visualize_clusters
from core.ml.clustering.inference import fit_kmeans, normalize_features
from core.ml.scene.inference import classify_scene
import scipy.stats

st.set_page_config(page_title="–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ —Ñ–æ—Ç–æ", layout="wide", page_icon="üì∑")

with open("assets/style.css") as f:
    st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

st.markdown("""
    <script>
    document.body.classList.remove('theme-dark', 'theme-light');
    document.body.classList.add('theme-light');
    </script>
""", unsafe_allow_html=True)

analyze_tab, cluster_tab = st.tabs(["–ê–Ω–∞–ª–∏–∑", "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"])

with analyze_tab:
    st.sidebar.header("–û–ø—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞")
    analyze_brightness_opt = st.sidebar.checkbox("–Ø—Ä–∫–æ—Å—Ç—å", True)
    analyze_sharpness_opt = st.sidebar.checkbox("–†–µ–∑–∫–æ—Å—Ç—å", True)
    analyze_contrast_opt = st.sidebar.checkbox("–ö–æ–Ω—Ç—Ä–∞—Å—Ç", True)
    analyze_saturation_opt = st.sidebar.checkbox("–ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å", True)
    analyze_pose_opt = st.sidebar.checkbox("–ê–Ω–∞–ª–∏–∑ –ø–æ–∑—ã", False)
    analyze_composition_opt = st.sidebar.checkbox("–ö–æ–º–ø–æ–∑–∏—Ü–∏—è", value=True)
    apply_scene_filter_opt = st.sidebar.checkbox("–ü—Ä–∏–º–µ–Ω—è—Ç—å —Ñ–∏–ª—å—Ç—Ä –ø–æ –∂–∞–Ω—Ä—É", value=False)
    apply_scene_filter_scene_opt = st.sidebar.checkbox("–ü—Ä–∏–º–µ–Ω—è—Ç—å —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ü–µ–Ω–µ", value=False)

    ml_analysis_type = st.sidebar.radio(
        "ML-–∞–Ω–∞–ª–∏–∑:",
        ["YOLO (–¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤)"]
    )

    grid_type = st.sidebar.selectbox("–°–µ—Ç–∫–∞ –Ω–∞–ª–æ–∂–µ–Ω–∏—è", ["–ü—Ä–∞–≤–∏–ª–æ —Ç—Ä–µ—Ç–µ–π", "–ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ", "–¶–µ–Ω—Ç—Ä", "–ù–µ—Ç"], index=3)

    grid_type_map = {
        "–ü—Ä–∞–≤–∏–ª–æ —Ç—Ä–µ—Ç–µ–π": "Rule of Thirds",
        "–ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ": "Golden Ratio",
        "–¶–µ–Ω—Ç—Ä": "Center",
        "–ù–µ—Ç": "None"
    }

    input_mode = st.sidebar.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:", ["–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª", "–í–µ–±-–∫–∞–º–µ—Ä–∞"])
    quality_model_type = st.sidebar.radio("–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:", ["ML (–ø—Ä–∏–∑–Ω–∞–∫–∏)", "CNN (ResNet)"])

    image = None
    uploaded_file = None
    camera_image = None

    if input_mode == "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª":
        uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type=["jpg", "jpeg", "png"])
        if uploaded_file:
            image = Image.open(uploaded_file).convert("RGB")
    elif input_mode == "–í–µ–±-–∫–∞–º–µ—Ä–∞":
        camera_image = st.camera_input("–°–¥–µ–ª–∞–π—Ç–µ —Å–Ω–∏–º–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        if camera_image:
            image = Image.open(camera_image).convert("RGB")

    st.markdown("## üì∑ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏")
    st.markdown("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

    def show_metrics(metrics):
        items = list(metrics.items())
        for i in range(0, len(items), 3):
            cols = st.columns(3)
            for j, (name, (display, _)) in enumerate(items[i:i+3]):
                with cols[j]:
                    st.markdown(f"<div class='metric'><h3>{name}</h3><p>{display}</p></div>", unsafe_allow_html=True)

    if image is not None:
        image_np = np.array(image)
        image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
        detected_boxes = detect_objects(image_cv)

        metrics = {}

        if analyze_brightness_opt:
            val = analyze_brightness(image_cv)
            metrics["–Ø—Ä–∫–æ—Å—Ç—å"] = (f"{val:.2f}", val)
        if analyze_sharpness_opt:
            val = analyze_sharpness(image_cv)
            metrics["–†–µ–∑–∫–æ—Å—Ç—å"] = (f"{val:.2f}", val)
        if analyze_contrast_opt:
            val = analyze_contrast(image_cv)
            metrics["–ö–æ–Ω—Ç—Ä–∞—Å—Ç"] = (f"{val:.2f}", val)
        if analyze_saturation_opt:
            val = analyze_saturation(image_cv)
            metrics["–ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å"] = (f"{val:.2f}", val)

        # ML-–º–µ—Ç—Ä–∏–∫–∏
        is_blur, sharpness_blur = detect_blur(image_cv)
        metrics["–†–∞–∑–º—ã—Ç–∏–µ"] = (f"{'–î–∞' if is_blur else '–ù–µ—Ç'} (—Ä–µ–∑–∫–æ—Å—Ç—å: {sharpness_blur:.1f})", sharpness_blur)
        has_human, has_animal, has_tech, humans, animals, tech = detect_categories(detected_boxes)

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ü–µ–Ω—ã –∏ –∂–∞–Ω—Ä–∞
        scene_name, scene_prob = classify_scene(image)
        metrics["–°—Ü–µ–Ω–∞ (Places365)"] = (f"{scene_name} ({scene_prob:.2%})", scene_prob)
        try:
            genre = predict_genre_ml(image, image_cv)
            genre_ru = GENRE_MODEL_TO_RU.get(genre, '–î—Ä—É–≥–æ–µ')
            metrics["–ñ–∞–Ω—Ä"] = (genre_ru, 1)
        except Exception as e:
            genre = f"–û—à–∏–±–∫–∞: {e}"
            genre_ru = '–î—Ä—É–≥–æ–µ'
            metrics["–ñ–∞–Ω—Ä"] = (genre, 0)

        try:
            if quality_model_type == "ML (–ø—Ä–∏–∑–Ω–∞–∫–∏)":
                quality_score = predict_quality_ml(image_cv, image)
                metrics["–ö–∞—á–µ—Å—Ç–≤–æ (ML)"] = (f"{quality_score:.2f} / 10", quality_score)
                features = extract_features(image_cv, image)
                has_object = features[23] 
                explanations = explain_quality(features)
                if not has_object:
                    explanations = [exp for exp in explanations if "–∫–æ–º–ø–æ–∑–∏—Ü–∏" not in exp.lower() and "–æ–±—ä–µ–∫—Ç" not in exp.lower()]
            else:
                quality_score = predict_quality_cnn(image, image_cv)
                metrics["–ö–∞—á–µ—Å—Ç–≤–æ (CNN)"] = (f"{quality_score:.2f} / 10", quality_score)
        except Exception as e:
            metrics["–ö–∞—á–µ—Å—Ç–≤–æ (ML)"] = (f"–û—à–∏–±–∫–∞: {e}", 0)

        # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
        if analyze_composition_opt:
            grid_type_eng = grid_type_map.get(grid_type, "None")
            grid_type_for_composition = grid_type_eng.lower().replace(" ", "_")
            final_score, composition_tips, image_cv = analyze_composition(
                image_cv, detected_boxes, grid_type=grid_type_for_composition
            )
            metrics["–û—Ü–µ–Ω–∫–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏"] = (f"{final_score:.1f} / 100", final_score)
        
        pose_suggestions = None
        if analyze_pose_opt:
            image_cv = draw_pose(image_cv)
            pose_suggestions = analyze_pose(image_cv)

        col1, col2 = st.columns([1, 1])
        with col1:
            show_metrics(metrics)
            if quality_model_type == "ML (–ø—Ä–∏–∑–Ω–∞–∫–∏)" and 'explanations' in locals():
                st.markdown("### –ü–æ—á–µ–º—É —Ç–∞–∫–∞—è –æ—Ü–µ–Ω–∫–∞?")
                for exp in explanations:
                    st.markdown(f"<div class='suggestion'>{exp}</div>", unsafe_allow_html=True)
            if analyze_composition_opt:
                if 'composition_tips' in locals() and composition_tips and "–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏." not in composition_tips:
                    st.markdown("### –°–æ–≤–µ—Ç—ã –ø–æ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏")
                    for tip in composition_tips:
                        st.markdown(f"<div class='suggestion'>{tip}</div>", unsafe_allow_html=True)
                else:
                    st.markdown("### –°–æ–≤–µ—Ç—ã –ø–æ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏")
                    st.info("–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏.")
            if analyze_pose_opt:
                if 'pose_suggestions' in locals() and pose_suggestions:
                    st.markdown("### –°–æ–≤–µ—Ç—ã –ø–æ –ø–æ–∑–µ")
                    for tip in pose_suggestions:
                        st.markdown(f"<div class='suggestion'>{tip}</div>", unsafe_allow_html=True)
        with col2:
            image_cv_disp = image_cv.copy()
            if apply_scene_filter_opt and genre:
                image_cv_disp = apply_genre_filter(image_cv_disp, genre_ru)
            if apply_scene_filter_scene_opt and scene_name:
                image_cv_disp = apply_scene_filter(image_cv_disp, scene_name)
            image_cv_disp = draw_grid(image_cv_disp, grid_type_map.get(grid_type, "None"))
            final_image = cv2.cvtColor(image_cv_disp, cv2.COLOR_BGR2RGB)
            st.image(final_image, caption="–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_container_width=True)

    else:
        st.info("üì§ –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ —Å–¥–µ–ª–∞–π—Ç–µ —Å–Ω–∏–º–æ–∫ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞.")

with cluster_tab:
    st.markdown("## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º")
    st.markdown("""
    –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (jpg, png). –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º (—è—Ä–∫–æ—Å—Ç—å, —Ä–µ–∑–∫–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–∞—Å—Ç, –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å, —Å—Ä–µ–¥–Ω–∏–π —Ü–≤–µ—Ç, –æ–±—ä–µ–∫—Ç—ã, –∂–∞–Ω—Ä, –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞).
    –í—ã–±–µ—Ä–∏—Ç–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –∑–∞—Ç–µ–º –Ω–∞–∂–º–∏—Ç–µ '–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å'.
    """)
    if 'uploaded_files' not in st.session_state:
        st.session_state.uploaded_files = None
    if 'cluster_results' not in st.session_state:
        st.session_state.cluster_results = None

    uploaded_files = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", type=["jpg", "jpeg", "png"], accept_multiple_files=True, key="cluster_upload")
    n_clusters = st.number_input("–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", min_value=2, max_value=10, value=3, step=1)
    cluster_btn = st.button("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å")
    clear_btn = st.button("–û—á–∏—Å—Ç–∏—Ç—å")

    if clear_btn:
        st.session_state.uploaded_files = None
        st.session_state.cluster_results = None
        st.rerun()

    if uploaded_files:
        st.session_state.uploaded_files = uploaded_files
    else:
        uploaded_files = st.session_state.uploaded_files

    if uploaded_files and cluster_btn:
        images = []
        features_list = []
        filenames = []
        for file in uploaded_files:
            try:
                image = Image.open(file).convert("RGB")
                image_np = np.array(image)
                image_cv = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
                features = extract_features(image_cv, image)
                try:
                    genre = predict_genre_ml(image, image_cv)
                    genre_ru = GENRE_MODEL_TO_RU.get(genre, '–î—Ä—É–≥–æ–µ')
                    features[19] = GENRE_TO_IDX.get(genre_ru, GENRE_TO_IDX['–î—Ä—É–≥–æ–µ'])
                except Exception:
                    features[19] = GENRE_TO_IDX['–î—Ä—É–≥–æ–µ']
                images.append(image)
                features_list.append(features)
                filenames.append(file.name)
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file.name}: {e}")
        features_arr = np.vstack(features_list)
        normed_features, scaler = normalize_features(features_arr)
        kmeans = fit_kmeans(normed_features, n_clusters=n_clusters, save_path="core/ml/clustering/kmeans_model.pkl")
        labels = kmeans.labels_
        st.success(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ core/ml/clustering/kmeans_model.pkl")

        plot_path = "core/ml/clustering/cluster_plot.png"
        visualize_clusters(normed_features, labels, save_path=plot_path)
        st.image(plot_path, caption="–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (PCA)", use_container_width=True)

        st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
        for cluster_id in range(n_clusters):
            idxs = [i for i, lbl in enumerate(labels) if lbl == cluster_id]
            if not idxs:
                continue
            mean_feat = features_arr[idxs].mean(axis=0)
            brightness = mean_feat[0]
            contrast = mean_feat[1]
            saturation = mean_feat[2]
            mean_r, mean_g, mean_b = mean_feat[3:6]
            colorfulness, std_rg, std_rb, std_gb = mean_feat[6:10]
            sharp_grid_mean, sharp_grid_std, sharp_grid_min, sharp_grid_max = mean_feat[10:14]
            min_dist_center, min_dist_thirds = mean_feat[14:16]
            n_humans = int(np.sum(features_arr[idxs, 16]))
            n_animals = int(np.sum(features_arr[idxs, 17]))
            n_tech = int(np.sum(features_arr[idxs, 18]))
            # –∂–∞–Ω—Ä –ø–æ –º–æ–¥–µ
            genre_idxs = [int(features_arr[idx, 19]) for idx in idxs]
            genre_idx = int(scipy.stats.mode(genre_idxs, keepdims=False).mode)
            hist_bins = mean_feat[20:23]
            has_object = int(np.sum(features_arr[idxs, 23]))
            comp_score = mean_feat[24]

            color_desc = f"R={mean_r:.0f}, G={mean_g:.0f}, B={mean_b:.0f}"
            genre_desc = GENRE_LABELS[genre_idx] if genre_idx < len(GENRE_LABELS) else "–î—Ä—É–≥–æ–µ"
            hist_desc = f"–¢–µ–Ω–∏: {hist_bins[0]:.2f}, –°—Ä–µ–¥–Ω–∏–µ: {hist_bins[1]:.2f}, –°–≤–µ—Ç–∞: {hist_bins[2]:.2f}"
            desc = (
                f"–Ø—Ä–∫–æ—Å—Ç—å: {brightness:.1f}, –†–µ–∑–∫–æ—Å—Ç—å: {sharp_grid_mean:.1f}, –ö–æ–Ω—Ç—Ä–∞—Å—Ç: {contrast:.1f}, "
                f"–ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å: {saturation:.1f}, –¶–≤–µ—Ç: {color_desc}, "
                f"–õ—é–¥–µ–π: {n_humans}, –ñ–∏–≤–æ—Ç–Ω—ã—Ö: {n_animals}, –¢–µ—Ö–Ω–∏–∫–∏: {n_tech}, "
                f"–ñ–∞–Ω—Ä: {genre_desc}, {hist_desc}"
            )
            st.markdown(f"#### –ö–ª–∞—Å—Ç–µ—Ä {cluster_id+1}: \n {desc}")
            cols = st.columns(5)
            for i, idx in enumerate(idxs):
                with cols[i % 5]:
                    st.image(images[idx], caption=filenames[idx], use_container_width=True)
    elif cluster_btn:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")

